{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "image_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "My7zPE4jB0nu",
        "HZNW4_mdOhmE",
        "PEJghYZiOOPL",
        "qtKxN8FXp83N",
        "nPD85x9WgkXc",
        "sUufLNdNHcbr",
        "j8MQwpS6l8fF"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My7zPE4jB0nu"
      },
      "source": [
        "<a name='main'></a>\n",
        "# **AI IN PRACTICE : HOW TO TRAIN AN IMAGE CLASSIFIER**\n",
        "### **Author: Sheetal Reddy**\n",
        "### **Contact : sheetal.reddy@ai.se**\n",
        "\n",
        "---\n",
        "**Introduction** \n",
        "\n",
        "The training \"AI in Practice\" will give you, at a basic level, knowledge about how to train a pre-trained model, the pre-requisites, what techniques that are used and how to continue experimenting in the finetuning of the model. \n",
        "\n",
        "In this training we are going to use image classification, open data-set, a pre-trained model and Colab* to train your model. \n",
        "\n",
        "A pre-trained model gives you the possibility to finetune an existing model trained on a large amount of data to better fit your purposes and by that also save you time. We will go through the more of the advantages later in the training. \n",
        "\n",
        "There are many pre-trained models available for different purposes you can find some of them here: https://pytorch.org/docs/stable/torchvision/models.html\n",
        "\n",
        "**The objective**\n",
        "\n",
        "The objective of this training is to give you enough knowledge to feel confident when entering an AI project. \n",
        "By understanding the steps requerired to train a model you will an advantage when working in AI related project. \n",
        "This by giving you both an theoretical knowledge but also by you being able to practice hands on - how to train a model.  \n",
        "\n",
        "**Learning objectives**\n",
        "\n",
        "After the training you will be able to: \n",
        "* Describe the necessary steps to train a model\n",
        "* Use a Jupyter Notebook - Google Colab\n",
        "* Be able to train a model \n",
        "  - Prepare datasets\n",
        "  - Finetune pre-trained models\n",
        "  - Visualize and quantify results\n",
        "\n",
        "**Pre-requisites**\n",
        "\n",
        "To be able to get the most out of this training we expect you to be aware of: \n",
        "\n",
        "*   The subject of AI \n",
        "*   The importance of data \n",
        "\n",
        "**Training instructions**\n",
        "\n",
        "The training is primarly performed individially but you will be placed in a group.\n",
        "\n",
        "There will be some group questions and exercises but you are expected to performe the tasks your-self.\n",
        "\n",
        "There is a Common Terminology section in the end of your Colab document. The concepts or wording available in the Common Terminology section will be marked with an (*)   \n",
        "\n",
        "There are also some links in the document if you want to learn more in the different sections\n",
        "\n",
        "Let us know if you have any questions or your group members – **but first google it!** \"Googling \" is one of the most common ways that data scientists work with understanding new techniques and ways of working.\n",
        "\n",
        "**Duration**\n",
        "*  Expected time to finish the training is in total 3 hours. \n",
        "\n",
        "**The challenge** \n",
        "\n",
        "*  The challenge in this training, is to finetune the pre-trained model to the use case and dataset - capable of **image classification**, see below for explanation. We will also later on in this training go through more on the benefits of working with a pre-trained model. \n",
        "*   In this case you will work with improving/training the model using a data set containing different images including scenes.\n",
        "*   The outcome of your work will result in a model that can classify \"nature scenes\" with a higher accuracy.\n",
        "\n",
        "**Image classification**\n",
        "\n",
        "\n",
        "So why did we choose image classification for this training? \n",
        "*  Image classification is a technique that is used to classify or predict the class of a specific object in an image. Image classification is one of the most important applications of computer vision. The main goal of this technique is to accurately identify the features in an image.  Its applications range from classifying objects in self-driving cars to identifying blood cells in the healthcare industry, from identifying defective items in the manufacturing industry to build a system that can classify persons wearing masks or not.\n",
        "\n",
        "*  Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do. To learn more: \n",
        "https://en.wikipedia.org/wiki/Computer_vision#:~:text=Computer%20vision%20is%20an%20interdisciplinary,human%20visual%20system%20can%20do.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDnHTLZeQPFe"
      },
      "source": [
        "# **Lets start the training !**  \n",
        "\n",
        "**How to train a pre-trained model**\n",
        "\n",
        "To train a model you usually need to plan according to the following steps below. The first three steps will set the foundation for what you will be able to train your model on and what results you will be able to expect. \n",
        "\n",
        "We will use this structure and go through the steps in the training one by one.\n",
        "\n",
        "1.  [Define adequately our problem (objective, desired outputs…).](#main) \n",
        "2. [Setup the computing environment](#computing_env)\n",
        "3. [Gather data](#computing_env) \n",
        "4. [Prepare the data](#data_preparation)\n",
        "5. [Train the model and choose a measure of success.](#training) - In this training the measure of succes is to have a model with a low error rate.\n",
        "6. [An overview of how a model learns](#results)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbKfPO_KAmI0"
      },
      "source": [
        "\n",
        "## **1.Define the problem**\n",
        "\n",
        "A problem well defined is a problem half-solved. \n",
        "\n",
        "Understanding the problem and developing the requirements isn't something you typically get right on the first attempt; this is often an iterative process where we initially define a set of rough requirements and refine the detail as we gain more information. \n",
        "\n",
        "By asking and aswering the five questions you are in a good way to be able to define a problem. \n",
        "\n",
        "1. What is the nature of the problem that requires solving?\n",
        "2. Why does the problem require a solution?\n",
        "3. How should solutions to the problem be approached?\n",
        "4. What aspect of the problem will a deep learning model solve?\n",
        "5. How is the solution to the problem intended to be interacted with?\n",
        "\n",
        "Since we already have a defined problem or in this case a challenge: **To finetune a pre-trained model with the aim to classify \"scenes\" with a high accuracy.**\n",
        "\n",
        "We will go ahead with setting the computing environment\n",
        "\n",
        "<a name='computing_env'></a>\n",
        "## **2. Setting up the computing environment**\n",
        "\n",
        "**Change the runtime setting of your colab notebook to GPU*:**\n",
        "Graphics Processing Units (GPUs), computing power, can significantly accelerate the training process for many deep learning models. Training models for tasks like image classification, video analysis, and natural language processing involves compute-intensive matrix multiplication and other operations that can take advantage of a GPU's massively parallel architecture.\n",
        "\n",
        "Training a deep learning model that involves intensive compute tasks on extremely large datasets can take days to run on a single processor. However, if you design your program to offload those tasks to one or more GPUs,  you can reduce training time to hours instead of days.\n",
        "\n",
        "**How to change your runtime setting to GPU* in your environment**\n",
        "\n",
        "The first thing you want to do is to in this Colab page go to the menubar and follow the following steps \"Körning > Ändra körningstyp > Välj \"GPU\". This will set the google colab environment up with a free GPU that will be used to train your models. If you have CPU selected it will still work, only much slower.\n",
        "\n",
        "## **3. Gather the Dataset**\n",
        "\n",
        "Gathering and preparing data requires great care. It usally involves taking below steps into considaration.  \n",
        "\n",
        "1. Determine what information you want or need to Collect to solve the problem\n",
        "2. Set a timeframe for data collection\n",
        "3. Determine your data collection method\n",
        "4. Collect the data\n",
        "5. Analyze the data and implement your findings\n",
        "\n",
        "The correct gathering of data is completely dependent on the problem you would like or need to solve. \n",
        "\n",
        "**Domain of the problem**\n",
        "\n",
        "Depending upon the domain of your problem, you may either use standard datasets collected by others or start collecting your own data. As you intend to use neural networks, then you should be aware that your dataset should be large, or else those techniques may not be very useful.\n",
        "What is the domain of your problem? Is it related to Computer Vision, Natural Language Processing, Sensor data, or some XYZ?\n",
        "\n",
        "In our case its related to Computer Vision for that reason we need to gather a large set of images. There are various ways to gather image data and you need to specify what images that are relevant for solving the problem.  \n",
        "\n",
        "It is important to plan ahead on how much data one may acquire. You cannot just store in a hard-disk and save it in directories and assume you are ready to go. A lot of effort goes in data storage, organization, annotation and pre-processing. \n",
        "\n",
        "**Data Privacy** \n",
        "\n",
        "Data privacy is an important part if individual people’s personal information is to be stored. Some data can be stored in simple text files but for other you may want to develop a database (or a light version) for faster access. If the data is too big to fit in memory, then big data techniques may need to be adopted (e.g. Hadoop framework). \n",
        "\n",
        "For this training we chosen not to include any personal data and we have also chosen to a pretty small dataset so its possible to store in a laptop. You will learn more about the data for this training as we go along the training. \n",
        "\n",
        "\n",
        "**Instructions to add the dataset to your drive**\n",
        "\n",
        "1. Download the dataset from the dropbox folder by clicking here\n",
        "https://www.dropbox.com/s/gf6d2t1zbogjjgg/AI_IN_PRACTICE.zip?dl=1\n",
        "2. Upload the **AI_IN_PRACTICE.zip** file to your google drive. \n",
        "3. Make sure you have a file called **AI_IN_PRACTICE.zip** in your **Mydrive** (In swedish **Min enhet**) in google drive \n",
        "\n",
        "You will learn about the data traits later in the training. \n",
        "\n",
        "Now you are all set to start running the code cells one by one ! The cells are they grey \"boxes\" that you will find throughout the Colab document. The fast and cool way to run a cell is to press shift+enter/ctrl + enter. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxGJoOtiuKTy"
      },
      "source": [
        "#The code in this cell connects your google drive space to the jupyter notebook and sets up fastai in your colab environment.\n",
        "#This will enable the code in your jupyter notebook to access the dataset in your google drive. \n",
        "\n",
        "#Install fastbook(contains fastai setup) in the colab environment. \n",
        "!pip install -Uqq torchtext==0.8.1\n",
        "!pip install -Uqq fastbook\n",
        "\n",
        "#Importing fastai into the jupyter notebook\n",
        "import fastbook\n",
        "\n",
        "#setup fastai and mounts your google drive space in /content/gdrive \n",
        "fastbook.setup_book()\n",
        "\n",
        "print('Setup complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F3Xh9kDUFgs"
      },
      "source": [
        "Now your google drive is mounted at /content/gdrive/MyDrive. It is only accesable through your Jupyter notebook for your view. \n",
        "Click on the above link to make sure your drive is mounted in the right location.\n",
        "\n",
        "If you experince any error,  let the organizer know.\n",
        "\n",
        "\n",
        "Now you should run the next cell to unzip/extract the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA0XDXtSFGMM"
      },
      "source": [
        "#When pressing the run button the code in this cell will unzip the AI_IN_PRACTICE.zip dataset and create a scenes folder in your google drive in MyDrive.\n",
        "\n",
        "#The code below Unzips the AI_IN_PRACTICE.zip file\n",
        "!unzip -q '/content/gdrive/MyDrive/AI_IN_PRACTICE.zip' -d '/content/gdrive/MyDrive/'\n",
        "print('The unzip is complete now and you can move to the next cell !')\n",
        "#This might take a while - Do not rerun the cell in between\n",
        "#When the code is executed correctly you will see this message \"The unzip is complete now and you can move to the next cell !\"\n",
        "#If you still do a rerun you will get the following message: \"replace /content/gdrive/MyDrive/AI_IN_PRACTICE/scenes/train/sea/1.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename:\" press \"A\" and press Enter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qXuYegiBZfM"
      },
      "source": [
        "Now we have the unziped dataset in the location /content/gdrive/MyDrive/AI_IN_PRACTICE/scenes\n",
        "\n",
        "Click on the link above to make sure you have scenes folder in your MyDrive. You should be able to see the different folders in the scenes dataset such as models, train, train_medium and valid.\n",
        "\n",
        "If you experience any error when you click the link, it means that the dataset is not at the right location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWRO402ozZvB"
      },
      "source": [
        "**Import the necessary packages**\n",
        "\n",
        "In python, which fastai* uses as a building block, we import packages (containing code) to our code using import statement as shown below for eg : import os \n",
        "\n",
        "It is a convinient way to import all the open source packages that are interesting and important for solving the challenge. There are many open source packages being produced and which ones to use for the specific problem needs to be explored. \n",
        "\n",
        "The importance of the packages we are using are described below in the code cell. We are going to work with the fastai libary which sits on top of PyTorch*. The fastai libary provides many useful functions that enable us to quickly and easily build neural networks  (NN) and train our models. To learn more about NN please watch the move through this link: https://www.youtube.com/watch?v=bfmFfD2RIcg\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFy0HsnnvD1T"
      },
      "source": [
        "\n",
        "#The code in this cell imports all the necesssary packages useful for training your model.\n",
        "\n",
        "from fastbook import *\n",
        "# imports fastai vision package to work with images \n",
        "from fastai.vision.all import *\n",
        "\n",
        "# imports fastai metrics like error_rate\n",
        "from fastai.metrics import error_rate # 1-accuracy\n",
        "\n",
        "#import numpy libraries for matrix manipulations \n",
        "import numpy as np \n",
        "\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "#import plotting and visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#import libraries to read and write images\n",
        "import cv2  \n",
        "     \n",
        "matplotlib.rc('image', cmap='Greys')\n",
        "print('Good Job ! You are on the right track')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9prvwMwNAXy"
      },
      "source": [
        "<a name='data_preparation'></a>\n",
        "# **4. Data Preparation**\n",
        "\n",
        "Data preparation is the process of cleaning and transforming raw data prior to processing and analysis. It is an important step prior to processing and often involves reformatting data, making corrections to data and the combining of data sets to enrich data.\n",
        "\n",
        "Data preparation is often a lengthy undertaking for data professionals or business users, but it is essential as a prerequisite to put data in context in order to turn it into insights and eliminate bias resulting from poor data quality.\n",
        "\n",
        "For example, the data preparation process usually includes standardizing data formats, enriching source data, and/or removing outliers.\n",
        "\n",
        "Dataset preparation can be divided into five steps \n",
        "\n",
        "1. [Data Exploration](#data_exploration)\n",
        "2. [Data Cleaning](#data_cleaning) \n",
        "3. [Data Augmentation](#data_augmentation)\n",
        "4. [Data Splitting](#data_splitting)\n",
        "5. [Visualize data](#data_visualization) \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl3djLbnPGcw"
      },
      "source": [
        "<a name='data_exploration'></a>\n",
        "## **4.1. Data Exploration**\n",
        "\n",
        "In the data exploration stage, we understand and try to answer some basic questions about the dataset. The question are listed below and are there for you to get a fast overview of the dataset you're handeling. In some cases this will give you enough information to understand if your dataset will be able to solve your problem or not. \n",
        "\n",
        "1. How big is the dataset? \n",
        "2. How many train files and validation/test files do we have?\n",
        "3. How many classes are there in the dataset ?\n",
        "4. How many data samples are there per class ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woWsrvjnOtAM"
      },
      "source": [
        "To be able to answer the above questions, we need to let our code know where our dataset is located. We do that by running the below code cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FbGKUdsmfi1"
      },
      "source": [
        "**Location of Scenes Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwSsaWovFvxB"
      },
      "source": [
        "#The code in this cell adds the location where the data exists to a path variable.\n",
        "path = '/content/gdrive/MyDrive/AI_IN_PRACTICE/scenes'\n",
        "print('Cell execution Completed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnWR9_iDNh5m"
      },
      "source": [
        "#The code in this cell stores all the locations of the train and test images in the dataset. \n",
        "\n",
        "#gets image locations from scenes/train folder and save them to train_files \n",
        "train_files=get_image_files(path+'/train')\n",
        "\n",
        "#get image locations from scenes/valid  folder and save them  to test_files \n",
        "test_files=get_image_files(path+'/valid')\n",
        "\n",
        "print('Cell execution Completed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W7NzNBIItol"
      },
      "source": [
        "If you need more information about the code in the code cells, Use doc() for more documentation. An example of how to use doc() is given below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpq72tr1HJi_"
      },
      "source": [
        "doc(get_image_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR0ktiFznEP5"
      },
      "source": [
        "**Amount of files in the scenes dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCPKML6vqx7z"
      },
      "source": [
        "#The code in this cell prints the number of images used for training and test/validation. The numbers are fixed to the dataset.\n",
        "print('Number of images used for training   '+ str(len(train_files)))\n",
        "print('Number of images used for validation   '+ str(len(test_files)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej2BcyK_nQGO"
      },
      "source": [
        "**Amount of Classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6AVIqxp684G"
      },
      "source": [
        "#The code in this cell prints the classes in our dataset\n",
        "labels = os.listdir(path+'/train')\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVdO-i20rzZt"
      },
      "source": [
        "#The code in this cell counts the number of samples per class in the train dataset. Plotted blow in the chart. \n",
        "counts = [0]*len(labels)\n",
        "for i in train_files:\n",
        "  for j in range(0,len(labels)):\n",
        "    if labels[j] in str(i):\n",
        "      counts[j]= counts[j]+1\n",
        "print('Counts extracted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgPVTF-y-Tff"
      },
      "source": [
        "#The code below defines a function for plotting the number of samples per class\n",
        "def plot_bar_counts():\n",
        "    # this is for plotting purpose\n",
        "    index = np.arange(len(labels))\n",
        "    plt.bar(labels, counts)\n",
        "    plt.xlabel('labels', fontsize=5)\n",
        "    plt.ylabel('No of data samples', fontsize=15)\n",
        "    plt.xticks(index, labels, fontsize=15, rotation=30)\n",
        "    plt.title('Train data analysis')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ou4OuAV-3Wy"
      },
      "source": [
        "#Plots the bar code of the training samples\n",
        "plot_bar_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sVn6hKfTp4o"
      },
      "source": [
        "\n",
        "#[**4.2. Data Cleaning**](#data_cleaning) \n",
        "\n",
        "In this training , we will do the data cleaning in the next pilot session. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oZuWgSwNb9y"
      },
      "source": [
        "<a name='data_augmentation'></a>\n",
        "## **4.3. Data Augmentation**\n",
        "\n",
        "Data augmentation is the technique of increasing the size of data used for training a model but also to create real life situations. For reliable predictions, the deep learning models often require a lot of training data, which is not always available. Therefore, the existing data is augmented in order to make a better generalized model.\n",
        "\n",
        "Although data augmentation can be applied in various domains, it's commonly used in computer vision. Some of the most common data augmentation techniques used for images are:\n",
        "\n",
        "**Position augmentation**\n",
        "*   Scaling\n",
        "*   Cropping\n",
        "*   Flipping\n",
        "*   Padding\n",
        "*   Rotation \n",
        "*   Translation \n",
        "*   Affine tranformation (ex:warping)\n",
        "\n",
        "**Color augmentation**\n",
        "*   Brightness\n",
        "*   Contrast \n",
        "*   Saturation \n",
        "*   Hue\n",
        "\n",
        "**Fun fact**: Color augmentations are the basis for the  **Instagram filters** we use to make us look picture perfect :) \n",
        "\n",
        "Below we go through some of the techniques and visualize different augmentations using one sample image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGqbhOnu3KxC"
      },
      "source": [
        "import random\n",
        "\n",
        "num = random.randint(0, len(train_files)-1)\n",
        "\n",
        "#Load a random image to visiaulize the image augmentations\n",
        "img = PILImage(PILImage.create(train_files[num]))\n",
        "\n",
        "#show the image\n",
        "show_image(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx-GsoT73EuH"
      },
      "source": [
        "## **Random Crop Augmentaion**\n",
        "\n",
        "Random crop is a data augmentation technique wherein we create a random subset of an original image. This helps our model generalize better because the object(s) of interest we want our models to learn are not always wholly visible in the image or the same scale in our training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYPznC562yUP"
      },
      "source": [
        "# The code in this cell applies Randomized crop to the image loaded above\n",
        "'''\n",
        "RandomResizedCrop(n): Randomly crops an image to size (nxn)\n",
        "'''\n",
        "n=224\n",
        "crop = RandomResizedCrop(n)\n",
        "_,axs = plt.subplots(3,3,figsize=(9,9))\n",
        "for ax in axs.flatten():\n",
        "    cropped = crop(img)\n",
        "    show_image(cropped, ctx=ax);\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78T2DCOr8E9p"
      },
      "source": [
        "## **Crop pad**\n",
        "\n",
        "Crop Pad is an additional augmentaqtion technique to increase the scenes data set by padding an image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjpCBNScszDN"
      },
      "source": [
        "# The code in this cell applies crop_pad to the image loaded above  \n",
        "_,axs = plt.subplots(1,3,figsize=(12,4))\n",
        "for ax,sz in zip(axs.flatten(), [150, 300, 500]):\n",
        "    show_image(img.crop_pad(sz), ctx=ax, title=f'Size {sz}');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVLsT2NZIJUp"
      },
      "source": [
        "## **Rotation Augmentation**\n",
        "\n",
        "A source image is random rotated clockwise or counterclockwise by some number of degrees, changing the position of the object in frame. \n",
        "Random Rotate is a useful augmentation in particular because it changes the angles that objects appear in your dataset during training. Random rotation can improve your model without you having to collect and label more data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZh9LrFZ8pXZ"
      },
      "source": [
        "# The code in this cell applies given rotations the image.\n",
        "\n",
        "timg = TensorImage(array(img)).permute(2,0,1).float()/255.\n",
        "def _batch_ex(bs): return TensorImage(timg[None].expand(bs, *timg.shape).clone())\n",
        "\n",
        "'''\n",
        "\n",
        "thetas - Angles which the original image is rotated to.\n",
        "\n",
        "For ex: thetas = [-15,0,15]\n",
        "\n",
        "Displays three images rotated to -15 degrees, 0 degrees and 15 degrees respectively\n",
        "\n",
        "'''\n",
        "thetas = [-30,-15,0,15,30]\n",
        "imgs = _batch_ex(5)\n",
        "deflt = Rotate()\n",
        "listy = Rotate(p=1.,draw=thetas)\n",
        "show_images( listy(imgs) ,suptitle='Manual List Rotate',titles=[f'{i} Degrees' for i in thetas])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZNW4_mdOhmE"
      },
      "source": [
        "## **Warping Augmentation**\n",
        "\n",
        "Appling warping technique adds distorted images to the scenes dataset.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3gbR1HOOmcx"
      },
      "source": [
        "scales = [-0.4, -0.2, 0., 0.2, 0.4]\n",
        "imgs=_batch_ex(5)\n",
        "vert_warp = Warp(p=1., draw_y=scales, draw_x=0.)\n",
        "horz_warp = Warp(p=1., draw_x=scales, draw_y=0.)\n",
        "show_images( vert_warp(imgs) ,suptitle='Vertical warping', titles=[f'magnitude {i}' for i in scales])\n",
        "show_images( horz_warp(imgs) ,suptitle='Horizontal warping', titles=[f'magnitude {i}' for i in scales])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0cvDs2_l2DS"
      },
      "source": [
        "**Flip**\n",
        "\n",
        "Flips a batch of images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it86223Ml-FK"
      },
      "source": [
        "with no_random(32):\n",
        "    imgs = _batch_ex(2)\n",
        "    deflt = Flip()\n",
        "    show_images( deflt(imgs) ,suptitle='Default Flip')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFVPbT_tYaTA"
      },
      "source": [
        "Let's now batch all these augmentation/transformation together and apply them in the code cell below. \n",
        "\n",
        "We also change the size of the images to make sure every image is of the same shape and size (normalize). This allows the GPU to apply the same instructions on all the images. \n",
        "\n",
        "When we normalize the images, the pixel channels standard deviations are reduced to help train models. If you do have problems training your model, one thing to do is check if you have normalized it. \n",
        "\n",
        "***NOTE: The types of data augmentations are very specific to the dataset. In our case we only rotate the image by a smaller degree to maintain representability of the real world. If we consider Medical images (Ex:cell Images), It is okay to rotate them by a larger degree(ex: 180 degrees)***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NnbJwZXjtCm"
      },
      "source": [
        "#tfms = None\n",
        "#The code in this cell collects all the data augmentations into one variable which can be applied to our dataset in the later stages.\n",
        "tfms =[*aug_transforms(size=224, min_scale=0.75, max_rotate=10, max_zoom=1.05, max_warp=.1, do_flip=True), Normalize.from_stats(*imagenet_stats)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFKPgy2lT-lx"
      },
      "source": [
        "#if you are running on GPU instance , this code cell will work, Otherwise it will throw an error ! \n",
        "#if you are not running on GPU, comment the second line (y = y.to(device=torch.device(\"cuda:0\"))). \n",
        "y = _batch_ex(9)\n",
        "y = y.to(device=torch.device(\"cuda:0\"))\n",
        "for t in tfms: y = t(y, split_idx=0)\n",
        "_,axs = plt.subplots(1,5, figsize=(12,3))\n",
        "for i,ax in enumerate(axs.flatten()):\n",
        "   show_image(y[i], ctx=ax)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEJghYZiOOPL"
      },
      "source": [
        "<a name='data_splitting'></a>\n",
        "## **4.4 Data Splitting**\n",
        "\n",
        "Now its time to split your data for training and validation. The training data usually contains 70% of the image dataset and the trainingValidation dataset the remaining 30%. \n",
        "\n",
        "Run the code below to perform the splitting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i28qA_ojxiN"
      },
      "source": [
        "#The code in this cell loads the whole train and valid images into a data variable. Also applies the tfms variable that we created in the previous cells. \n",
        "np.random.seed(42)\n",
        "\n",
        "'''\n",
        "The method below loads train and valid subfolders in the code  (data =)\n",
        "\n",
        "train : name of the  train subfolder \n",
        "valid : name of the valid subfolder\n",
        "item_tfms : transforms performed on the individual image\n",
        "batch_tfms : transforms performed on the batch \n",
        "bs : batch size\n",
        " \n",
        "'''\n",
        "data = ImageDataLoaders.from_folder(path,train='train', valid ='valid', item_tfms=Resize(224), batch_tfms=tfms, bs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbXH21ckhYZX"
      },
      "source": [
        "Before we move on to next code cell, we need to be clear with the below question. Believe me, Its Important ! \n",
        "\n",
        "**What Is  Batch Size?** \n",
        "\n",
        "To refresh you menemory please look at the video explaining NN here:  https://www.youtube.com/watch?v=bfmFfD2RIcg\n",
        "\n",
        "*  The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n",
        "\n",
        "*  Think of a batch as a for-loop iterating over one or more samples and making predictions. At the end of the batch, the predictions are compared to the expected output variables and an error is calculated. From this error, the update algorithm is used to improve the model, e.g. move down along the error gradient.\n",
        "\n",
        "*  A training dataset can be divided into one or more batches.\n",
        "*  Batch Size(bs) can be changed in this code cell [here](#data_splitting). It's value is currently set to 10.\n",
        "\n",
        "To get more information about a Batch Size please follow the link: https://www.youtube.com/watch?v=U4WB9p6ODjM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyRWzqHKVziY"
      },
      "source": [
        "## **4.5 Visualize Data**\n",
        "\n",
        "By Visualizing the data you can confirm that you are on the right track e.g. regarding the labeling \n",
        "\n",
        "Do your images match the correct labels? \n",
        "If yes, then you have succeeded ! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLyn8ahvpe0X"
      },
      "source": [
        "#The below line of code shows a random batch of images \n",
        "data.show_batch(figsize=(10,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtKxN8FXp83N"
      },
      "source": [
        "<a name='training'></a>\n",
        "\n",
        "# **5.Training/Fine-Tuning the model using Transfer Learning**\n",
        "\n",
        "**Welcome back!**\n",
        "\n",
        "Now we will start with fine-tuning of our pretrained model. This means that we are building a model which will take images as input and will output the predicted probability for each of the categories, in this case, it will get 6 probabilities and class with the maximun probability is chosen as the label. For this task we will use a technique called Transfer Learning.  To learn more about transfer learning please follow this link: https://www.youtube.com/watch?v=5T-iXNNiwIs\n",
        "\n",
        "\n",
        "**What is Transfer Learning?**\n",
        "\n",
        "\n",
        "*   Transfer learning is a technique where you use a model trained on a very large dataset (usually ImageNet in computer vision) and then adapt it to your own dataset.\n",
        "\n",
        "*  The idea is that the model has learned to recognize many features on all of this data, like ImageNet, and that you will benefit from this knowledge, especially if your dataset is small. \n",
        "\n",
        "*   In practice, you need to change the last part of the model to be adapted to your own number of classes. \n",
        "\n",
        "*   Most convolutional models end with a few linear layers (a part we will call the head).\n",
        "\n",
        "*   The last convolutional layer will have analyzed features in the image that went through the model, and the job of the head is to convert those in predictions for each of your classes. \n",
        "\n",
        "*   In transfer learning one keeps all the convolutional layers (called the body or the backbone of the model) with their weights pretrained on ImageNet but will define a new head initialized randomly.\n",
        "\n",
        "**Two-Phase Training of the model**\n",
        "*   We will train the model in two phases: first we freeze the body weights and only train the head (to convert those analyzed features into predictions for our own data). In the second phase we unfreeze the layers of the backbone (gradually if necessary) and fine-tune the whole model (possibly using differential learning rates).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BF9xv3Tqeqf"
      },
      "source": [
        "For this training we have chosen a pretrained model called resnet34, it has previously been trained on 1,5 million of images. This means that we don't have to start with a model that knows nothing, we start with a model that knows something about recognizing images already. The 34 stands for the number of layers in the network, a smaller model trains faster. There is a bigger version is called resnet50. \n",
        "\n",
        "With below code our model will be able to train with the resnet34."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbhv0IAsrLN_"
      },
      "source": [
        "#The code in this cell will use a cnn_learner method. With this line of code we tell a learner to create a cnn model for us, in this case it's a resnet34. \n",
        " \n",
        "#The cnn_learner method helps you to automatically get a pretrained model from a given architecture, in this case resnet34\n",
        "learn = cnn_learner(data, models.resnet34, loss_func=CrossEntropyLossFlat(), metrics=[error_rate, accuracy])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ICive0bXJIw"
      },
      "source": [
        "###  **Wait !!!**\n",
        "\n",
        "There seems to be a lot of terms in the code that are complicated in the previous cell. Let's review each of them a bit \n",
        "\n",
        "\n",
        "*  **CNN** : Convolutional Neural Networks are a class of neural networks that are widely used in the areas of images recognition, images classifications. Objects detections, recognition faces etc. A convolution is the  basic operation of a CNN. For more explanation, watch the below video.\n",
        "https://www.youtube.com/watch?v=YRhxdVk_sIs&t=419s\n",
        "\n",
        "*  **Cross-Entropy Loss**  : Cross-entropy loss is a loss function used for this dataset. It  has two benefits:\n",
        "\n",
        "> 1. It works even when our dependent variable has more than two categories.\n",
        "> 2. It results in faster and more reliable training.\n",
        "\n",
        "* **Error rate**: \n",
        "        error_rate = 1 - accuracy \n",
        "        accuracy = no of correctly classified samples / all samples\n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdvL0xSZz530"
      },
      "source": [
        "The below code-cell shows the detailed architecture of the deep neural network model(in our case resnet34) we are training. Knowing the architecture of a DNN(deep neural network) is useful in designing better neural network architectures for more advanced usecases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TQoY0Lqtu3W"
      },
      "source": [
        "#The code in this cell shows the architecture of the model( in our case CNN) that is being trained.\n",
        "learn.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nOYPoo1s3Cc"
      },
      "source": [
        "## **Phase 1: Finetune the head of the model**\n",
        "\n",
        "Now we enter the first phase of the training which means that we first we freeze the body weights and only train the head (to convert those analyzed features into predictions for our own data). We will train our models by letting it cycle through all our data 6 times. The number 6 is the number of times we let the model go through all the data. We can see the training loss which is telling us how much is the model learning from the data. The validation loss tells us how generalizable is the model.\n",
        "\n",
        "In both the cases, training and validation loss, it's good to have a decreasing trend.\n",
        "\n",
        "1 cycle = 1 epoch\n",
        "\n",
        "It will take sometime to train your model.\n",
        "\n",
        "Sit and relax after running the below cell ! :) You did a great job !\n",
        "\n",
        "Or you can read [here](#cycles)  on how to choose the number of cycles/epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYmBJQ67t1Tp"
      },
      "source": [
        "#The code in this cell will run the training job for 6 epochs.\n",
        "learn.fit_one_cycle(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwtCL9Y1uhDQ"
      },
      "source": [
        "Ideally if your model is learning something, you should see a certain trend. Your train_loss and valid_loss  and error_rate should be decreasing while accuracy should be increasing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH7GSbTsgiOR"
      },
      "source": [
        "#Plots the loss for both training  and validation dataset\n",
        "learn.recorder.plot_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cWknYP7G1g6"
      },
      "source": [
        "#The code in this code cell is saving the model to the disk with name stage-1\n",
        "learn.save('stage-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNOC389I8WFX"
      },
      "source": [
        "Observe the decreasing trend in the plots above !!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPD85x9WgkXc"
      },
      "source": [
        "\n",
        "<a name='cycles'></a>\n",
        "### **How do we select the number of epochs?**\n",
        "\n",
        "\n",
        "*  Often you will find that you are limited by time, rather than generalization and accuracy, when choosing how many epochs to train for. So your first approach to training should be to simply pick a number of epochs that will train in the amount of time that you are happy to wait for. Then look at the training and validation loss plots, as shown above, and in particular your metrics, and if you see that they are still getting better even in your final epochs, then you know that you have not trained for too long. In this situation you can increase the number of epochs you are training for.\n",
        "\n",
        "*  If you have the time to train for more epochs, you may also want to instead use that time to train more parameters—that is, use a deeper architecture.\n",
        "\n",
        "Now we successfully finetuned our model. In order not to lose our progress, let's save our trained model in preset location. The model will be saved on your google drive at /content/gdrive/MyDrive/scenes/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUufLNdNHcbr"
      },
      "source": [
        "## **Phase 2: Unfreezing and fine-tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MKOxXSi2Dsq"
      },
      "source": [
        "As mentioned above, training is a two-phase process. In the first training, we train only last layer of the model. It’ll never overfit and will give good results, but to really make the best use of the model, we unfreeze and fine tune all the layers in the model to train it better.\n",
        "\n",
        "Finetuning all the layers of the model let's the model weights of all the layers finetuned to the features of the scenes dataset. This makes the model perform better on the scenes dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2cXL8mzHg-c"
      },
      "source": [
        "#The code in this code cell unfreezes and trains the whole resnet34 model. We now allow for the whole model to be trained, not just the last layer. \n",
        "learn.unfreeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1j27SBO3Yfl"
      },
      "source": [
        "**Finding the best learning rate**\n",
        "\n",
        "Finding a good learning rate is one important problem faced by the machine learning community. Learning rate decides how fast should the model weights be updated. It is mostly trial and error based but fastai has come up with a tool called learning rate finder which can give us the most appropriate learning rate. \n",
        "\n",
        "For a more intuitive explanation on how the learning rate finder works, refer to the below link\n",
        "(https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html)\n",
        "\n",
        "The below cell plots a curve showing the learning late versus loss. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32j5pq5oHppb"
      },
      "source": [
        "#The code in the code cell runs the learning rate finder provided by fastai\n",
        "learn.lr_find()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDXlA2DA489O"
      },
      "source": [
        "Now you see a text above the plot which suggests a learning rate range. Change the lr_min value in the code cell below  to the suggested lr_min value in the plot.\n",
        "\n",
        "For example if you the Suggested LRs are given as below :\n",
        "\n",
        "SuggestedLRs(lr_min=0.004786301031708717, lr_steep=0.0014454397605732083)\n",
        "\n",
        "Then, change the lr_min value to 0.0047 below in the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BGfMisO42ad"
      },
      "source": [
        "#Change the value of lr_min to the value suggested in the previous plot.\n",
        "lr_min = 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUaqI6zp_Lsg"
      },
      "source": [
        "Now, we train the model again after unfreezing all the layers of the pretrained model and also using the learning rate from the learning rate finder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjgtDHysHrWj"
      },
      "source": [
        "#The code in the code cell here runs a training for 5 epochs.\n",
        "learn.fit_one_cycle(5, lr_max=slice(1e-6,lr_min))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB1ZJj7tciu0"
      },
      "source": [
        "Now we successfully finished phase-2 training of our model.\n",
        "\n",
        " In order not to lose our progress, let's save our trained model in preset location. The model will be saved on your google drive at /content/gdrive/MyDrive/scenes/models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsp4iQhkcVUe"
      },
      "source": [
        "#The code in this cell is saving the model to the disk with name stage-2\n",
        "learn.save('stage-2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8IliDueutKd"
      },
      "source": [
        "<a name='results'></a>\n",
        "# **Results Intepretation and Analysis**\n",
        "\n",
        "*Now comes the most interesting part!*\n",
        "\n",
        "\n",
        "\n",
        "We will first see which were the categories that the model was most confused with. We will try to see if what the model predicted is reasonable or not. Furthermore, we will plot a confusion matrix where we can see and learn more about the mistakes that the model made. We will explain the confusion matrix a bit further down. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMqltIknG9s3"
      },
      "source": [
        "#The code in this cell when exected performs an analysis of the model performance on all the classes. The results of the analysis are shown in the next code cells.\n",
        "interp = ClassificationInterpretation.from_learner(learn)\n",
        "\n",
        "losses,idxs = interp.top_losses()\n",
        "\n",
        "print('Interpretation and Analysis of Results done ! ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDiH-NdZoIQ1"
      },
      "source": [
        "# The code in this code cell shows some sample images, actual ground truth used for training and the predicted label.\n",
        "# If the predicted label and the ground truth match, the labels are shown in green.\n",
        "# If the predicted label and the ground truth do not match, the labels are shown in red.\n",
        "learn.show_results()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAzl5Qj7wNtr"
      },
      "source": [
        "So, one of the most interesting things we can do is called plot top losses. What this does is plot out when the model was very certain about a certain class, but was wrong. This means you are going to have a high loss. In other words; the model was confident about an answer, but answered wrong. The title of each image shows: prediction, actual, loss, probability of actual class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbBk9aWWHGn5"
      },
      "source": [
        "#The code in this cell shows the images the model is most confused on.\n",
        "\n",
        "'''For every image, it shows \n",
        "1. Prediction: The  label predicted by the model.\n",
        "2. Actual: The actual label in the dataset.\n",
        "3. Loss :  The cross entropy loss of the image. More loss means the model is very certain about a wrong prediction.\n",
        "4. Probability : How certain is the model's prediction\n",
        "\n",
        "'''\n",
        "interp.plot_top_losses(9, figsize=(15,11))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93exv01t0rRY"
      },
      "source": [
        "\n",
        "\n",
        "The confusion matrix is a way to visualuize your results and get an understanding for where your model makes mistakes and how frequent they are.\n",
        "\n",
        "\n",
        "The confusion matrix so interesting that we want everyone to understand it properly. We gather in the main group to discuss it.\n",
        " \n",
        "\n",
        " If you see that people are still working, grab a coffee and come back ! :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFwv5zhmHMbh"
      },
      "source": [
        "interp.plot_confusion_matrix(figsize=(12,12), dpi=60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8jZt1Rb1O6b"
      },
      "source": [
        "The most confused grabs the most common wrong predictions out of the confusion matrix. This can allow you for example as a domain expert, to understand based on your expertise, is this something that the model should be confused about. We can all understand that a glacier in many cases may be easy to confuse with mountains as glaciers many times exist in mountains. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5KBmN6ZHO7G"
      },
      "source": [
        "#The code in this code cell gives us the classes on which the model is confused in descending order.\n",
        "'''\n",
        "For example:\n",
        "('glacier', 'mountain', 131)\n",
        "\n",
        "What we can infer from the above line is that  131 glacier images have been predicted as mountain images.\n",
        "\n",
        "'''\n",
        "interp.most_confused(min_val=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8MQwpS6l8fF"
      },
      "source": [
        "## **Let's see if you can get  better Accuracy ! Try it out**\n",
        "\n",
        "\n",
        "<a name='data_cleaning'></a>\n",
        "## **Data Cleaning**\n",
        "\n",
        "Oops ! seems like the organizers have mixed up two datasets in rush :P. \n",
        "\n",
        " Can you try to clean it and see if that gives any accuracy gains ?\n",
        "\n",
        " **TIP** : The mix up happened with mostly the glacier and building classes.\n",
        "\n",
        " Other suggestions which might help in the accuracy gain:\n",
        "\n",
        "*  Use the train_medium dataset in  /content/gdrive/MyDrive/AI_IN_PRACTICE/scenes provided which has more data\n",
        "*  Increase the batch size "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRMymfu1FygD"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations!!**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "You have completed the training :) \n",
        "\n",
        "Please return to the main group. \n",
        "Please be ready to let us know your error rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UujAj8GwRbq"
      },
      "source": [
        "# **Common Terminology used in this training**\n",
        "\n",
        "*   **CPU**: A central processing unit, also called a central processor, main processor or just processor, is the electronic circuitry within a computer that executes instructions that make up a computer program. The CPU performs basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions in the program.\n",
        "\n",
        "*   **GPU**: A graphics processing unit, is a specialized, electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles. Modern GPUs are very efficient at manipulating computer graphics and image processing. Their highly parallel structure makes them more efficient than general-purpose central processing units (CPUs) for algorithms that process large blocks of data in parallel.\n",
        "\n",
        "*  **fastai** : is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. To learn more follow this link: https://docs.fast.ai/\n",
        "\n",
        "*   **PyTorch** : Is a Python-based scientific computing and deep learning framework. It's a replacement for NumPy to use the power of GPUs. It's a deep learning research platform that provides maximum flexibility and speed.\n",
        "\n",
        "*  **Google Colab** : At this moment you are in a google colab environment and you will be using this platform to run code and start learning about AI. Colab is a cloud based working environment that allows to to collaborate and train your models. A great environment to try things out and test. \n",
        "\n",
        "*  **Python** : Python is an interpreted programming language currently being used for any machine learning projects. Many of the open source Machine learning packages are extensively available in python only because of which it became a go-to language for Machine learning prototyping.\n",
        "\n",
        "* **epoch** : An epoch refers to one cycle of training through the full training dataset.\n",
        "\n",
        "*  **Imagenet**: ImageNet is a dataset consisting of 1.3 million images of various sizes around 500 pixels across, in 1,000 categories, which took a few days to train\n",
        "\n",
        "*  **Pretrained model** : The model that has been trained from scratch on a very large dataset(usually ImageNet in computer vision) is called the pretrained model. To learn more about pretrained models, check the link below.\n",
        "https://towardsdatascience.com/how-do-pretrained-models-work-11fe2f64eaa2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtNGYCl0CLvM"
      },
      "source": [
        "## **Acknowledgements**\n",
        "\n",
        "1. A huge thanks to Fastai for providing a framework for fast prototyping.\n",
        "2. Thanks to Kaggle and Intel for proving the scenes classification dataset"
      ]
    }
  ]
}